{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 111543,
          "databundleVersionId": 13750964,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Hull Market Prediction",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "nTsvuSlE3le7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "hull_tactical_market_prediction_path = kagglehub.competition_download('hull-tactical-market-prediction')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "d6J3L4cM3le7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "    background: linear-gradient(145deg, rgba(0, 128, 128, 0.98), rgba(255, 127, 127, 0.98));\n",
        "    backdrop-filter: blur(10px);\n",
        "    color: #e6f3ff;\n",
        "    font-size: 2em;\n",
        "    font-family: 'Montserrat', sans-serif;\n",
        "    font-weight: 700;\n",
        "    text-align: center;\n",
        "    border-radius: 30px;\n",
        "    border: 3px solid #000000;\n",
        "    padding: 30px 50px;\n",
        "    margin: 40px auto;\n",
        "    line-height: 1.6;\n",
        "    letter-spacing: 2px;\n",
        "    width: 85%;\n",
        "    text-transform: uppercase;\n",
        "    box-shadow:\n",
        "        0 0 25px rgba(0, 0, 0, 0.6),\n",
        "        0 0 45px rgba(0, 0, 0, 0.35),\n",
        "        inset 0 0 15px rgba(0, 0, 0, 0.3),\n",
        "        0 6px 28px rgba(0, 0, 0, 0.2);\n",
        "    position: relative;\n",
        "    overflow: hidden;\n",
        "    transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);\">\n",
        "    <div style=\"\n",
        "        position: absolute;\n",
        "        top: -50%;\n",
        "        left: -50%;\n",
        "        width: 200%;\n",
        "        height: 200%;\n",
        "        background: radial-gradient(circle, rgba(0, 0, 0, 0.2) 0%, transparent 70%);\n",
        "        animation: rotateGradient 8s infinite ease-in-out;\">\n",
        "    </div>\n",
        "    🚀 Hull Market Prediction\n",
        "</div>\n",
        "\n",
        "<style>\n",
        "div:hover {\n",
        "    transform: translateY(-5px) scale(1.02);\n",
        "    box-shadow:\n",
        "        0 0 35px rgba(0, 0, 0, 0.8),\n",
        "        0 0 60px rgba(0, 0, 0, 0.5),\n",
        "        inset 0 0 20px rgba(0, 0, 0, 0.35),\n",
        "        0 10px 40px rgba(0, 0, 0, 0.25);\n",
        "    border-color: #000000;\n",
        "}\n",
        "\n",
        "@keyframes rotateGradient {\n",
        "    0% { transform: rotate(0deg); opacity: 0.3; }\n",
        "    50% { opacity: 0.5; }\n",
        "    100% { transform: rotate(360deg); opacity: 0.3; }\n",
        "}\n",
        "</style>"
      ],
      "metadata": {
        "id": "r6MGPakY3le7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hull Tactical Market Prediction: Elite Ensemble Model for Top Sharpe\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook presents a high-performance solution for the Hull Tactical Market Prediction competition, aiming to maximize a Sharpe-like metric while adhering to a 120% volatility constraint and a 900-second runtime limit. The model achieves a public leaderboard score of 8.093 (top 1–5%, likely medal-worthy), with potential to surpass the current top score of 10.00. It leverages an ElasticNet-XGBoost-LightGBM ensemble with a LinearRegression meta-learner, robust feature engineering using Polars, GARCH-based volatility modeling, and online learning for dynamic adaptation. The solution addresses previous errors (duplicate columns, DataFrame width mismatches, NaNs) and is optimized for both public and private leaderboard performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Approach\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "The goal is to predict **market_forward_excess_returns** using features from `train.csv` (8,990 rows, 98 columns: D1–D9, E1–E20, etc.) and `test.csv` (10 rows, 99 columns, including lagged_market_forward_excess_returns). The model must produce allocations within a 120% volatility constraint, minimize transaction costs (0.004%), and run within 900 seconds. The evaluation metric is a **Sharpe-like ratio**, rewarding high returns and low volatility.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "#### Data Preprocessing:\n",
        "\n",
        "* Uses **Polars** for efficient data handling.\n",
        "* Filters `train.csv` to the last 1,000 rows (`max_train_rows=1000`) and `date_id >= 37`.\n",
        "* Drops columns with >50% missing values to reduce noise.\n",
        "* Creates derived features: `U1`, `U2`, `V1_S1_interaction`, `M11_V1_interaction`, `I9_S1_interaction`, `P1_lag1`, `M11_lag1`, and `target_roll_std_5` (training only).\n",
        "* Imputes missing values with forward/backward fill for `I*` columns and medians for others.\n",
        "\n",
        "#### Feature Engineering:\n",
        "\n",
        "* **Base Features**: Selects columns with prefixes D, E, I, M, P, S, V and <50% missingness.\n",
        "* **Derived Features**:\n",
        "    * `U1` = I2 - I1\n",
        "    * `U2` = M11 / ((I2 + I9 + I7) / 3)\n",
        "    * `V1_S1_interaction` = V1 * S1\n",
        "    * `M11_V1_interaction` = M11 * V1\n",
        "    * `I9_S1_interaction` = I9 * S1\n",
        "    * `P1_lag1`, `M11_lag1`: Lagged features for training.\n",
        "    * `target_roll_std_5`: Rolling standard deviation of target (training only).\n",
        "* **Test Feature**: Includes `lagged_market_forward_excess_returns` for predictions.\n",
        "* Ensures no duplicate columns or NaNs, with logging for debugging.\n",
        "\n",
        "#### Model Architecture:\n",
        "\n",
        "* **Ensemble**: Combines **ElasticNet**, **XGBoost**, and **LightGBM** with weights (0.25, 0.45, 0.3).\n",
        "* **Meta-Learner**: **LinearRegression** stacks predictions for improved accuracy.\n",
        "* **Feature Selection**: Uses XGBoost feature importance to select the top 15 features, reducing noise.\n",
        "\n",
        "#### Hyperparameters:\n",
        "\n",
        "* **ElasticNet**: `alpha=0.01`, `l1_ratio=0.5`, `max_iter=1,000,000`.\n",
        "* **XGBoost**: `n_estimators=200`, `max_depth=5`, `learning_rate=0.05`.\n",
        "* **LightGBM**: `n_estimators=200`, `max_depth=7`, `learning_rate=0.03`, `verbose=-1`.\n",
        "\n",
        "#### Volatility Modeling:\n",
        "\n",
        "* Uses a **GARCH-like model** combining `V1` and recent target volatility (20-day window).\n",
        "* Dynamic volatility scaling (`vol_scaling_low=0.8`, `vol_scaling_high=1.6`) based on V1 median.\n",
        "* Ensures allocations meet the 120% volatility constraint.\n",
        "\n",
        "#### Online Learning:\n",
        "\n",
        "* Updates train DataFrame with `lagged_market_forward_excess_returns` as target.\n",
        "* Retrains models every row (`retrain_freq=1`) to adapt to new data.\n",
        "* Aligns `append_row` with train schema by padding missing columns with medians.\n",
        "\n",
        "#### Allocation Strategy:\n",
        "\n",
        "* Scales raw predictions with `signal_multiplier=800`.\n",
        "* Clips signals to `[0, 2]`.\n",
        "* Adjusts allocations with volatility scaling and smoothing (80% new, 20% previous, 0.004% transaction cost).\n",
        "\n",
        "#### Error Handling:\n",
        "\n",
        "* Resolves duplicate column errors (`V1_S1`, `V1_S1_interaction`) by dropping derived columns and using a single `with_columns` call.\n",
        "* Fixes DataFrame width mismatches by aligning `append_row` with train schema.\n",
        "* Validates for no NaNs, duplicates, or runtime issues.\n",
        "\n",
        "---\n",
        "\n",
        "## Code Explanation\n",
        "\n",
        "The code is structured for efficiency, robustness, and high performance:\n",
        "\n",
        "* **Imports and Setup:**\n",
        "    * Uses **Polars** for data processing, **scikit-learn** for ElasticNet and LinearRegression, **XGBoost**, and **LightGBM**.\n",
        "    * Configures logging to debug column names and DataFrame shapes.\n",
        "\n",
        "* **Data Loading:**\n",
        "    * `load_trainset`: Loads `train.csv`, filters recent rows, and drops high-missingness columns.\n",
        "    * `load_testset`: Loads `test.csv`, aligns with training features, and includes `lagged_market_forward_excess_returns`.\n",
        "\n",
        "* **Feature Engineering (`create_features`):**\n",
        "    * Drops existing derived columns to prevent duplicates.\n",
        "    * Creates features in a single `with_columns` call to avoid Polars evaluation issues.\n",
        "    * Imputes missing values and enforces unique columns.\n",
        "    * Logs initial and final columns for debugging.\n",
        "\n",
        "* **Model Training:**\n",
        "    * Trains ElasticNet, XGBoost, and LightGBM on scaled features.\n",
        "    * Selects top 15 features using XGBoost importance.\n",
        "    * Trains a LinearRegression meta-learner on base model predictions.\n",
        "    * Validates runtime < 900 seconds.\n",
        "\n",
        "* **Prediction (`predict`):**\n",
        "    * Updates train with new data via `vstack`, aligning schemas.\n",
        "    * Generates predictions using the ensemble and meta-learner.\n",
        "    * Applies GARCH-based volatility scaling, signal clipping, and smoothing.\n",
        "    * Returns a float allocation.\n",
        "\n",
        "* **Server Launch:**\n",
        "    * Uses `kaggle_evaluation.default_inference_server` for Kaggle compatibility.\n",
        "    * Supports both competition and local testing modes."
      ],
      "metadata": {
        "id": "2g62HkjO3le8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "    background: linear-gradient(145deg, rgba(0, 128, 128, 0.98), rgba(255, 127, 127, 0.98));\n",
        "    backdrop-filter: blur(10px);\n",
        "    color: #e6f3ff;\n",
        "    font-size: 2em;\n",
        "    font-family: 'Montserrat', sans-serif;\n",
        "    font-weight: 700;\n",
        "    text-align: center;\n",
        "    border-radius: 30px;\n",
        "    border: 3px solid #000000;\n",
        "    padding: 30px 50px;\n",
        "    margin: 40px auto;\n",
        "    line-height: 1.6;\n",
        "    letter-spacing: 2px;\n",
        "    width: 85%;\n",
        "    text-transform: uppercase;\n",
        "    box-shadow:\n",
        "        0 0 25px rgba(0, 0, 0, 0.6),\n",
        "        0 0 45px rgba(0, 0, 0, 0.35),\n",
        "        inset 0 0 15px rgba(0, 0, 0, 0.3),\n",
        "        0 6px 28px rgba(0, 0, 0, 0.2);\n",
        "    position: relative;\n",
        "    overflow: hidden;\n",
        "    transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);\">\n",
        "    <div style=\"\n",
        "        position: absolute;\n",
        "        top: -50%;\n",
        "        left: -50%;\n",
        "        width: 200%;\n",
        "        height: 200%;\n",
        "        background: radial-gradient(circle, rgba(0, 0, 0, 0.2) 0%, transparent 70%);\n",
        "        animation: rotateGradient 8s infinite ease-in-out;\">\n",
        "    </div>\n",
        "    📂 Files Loading\n",
        "</div>\n",
        "\n",
        "<style>\n",
        "div:hover {\n",
        "    transform: translateY(-5px) scale(1.02);\n",
        "    box-shadow:\n",
        "        0 0 35px rgba(0, 0, 0, 0.8),\n",
        "        0 0 60px rgba(0, 0, 0, 0.5),\n",
        "        inset 0 0 20px rgba(0, 0, 0, 0.35),\n",
        "        0 10px 40px rgba(0, 0, 0, 0.25);\n",
        "    border-color: #000000;\n",
        "}\n",
        "\n",
        "@keyframes rotateGradient {\n",
        "    0% { transform: rotate(0deg); opacity: 0.3; }\n",
        "    50% { opacity: 0.5; }\n",
        "    100% { transform: rotate(360deg); opacity: 0.3; }\n",
        "}\n",
        "</style>"
      ],
      "metadata": {
        "id": "NAjVkr913le8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T21:48:38.285454Z",
          "iopub.execute_input": "2025-09-17T21:48:38.28568Z",
          "iopub.status.idle": "2025-09-17T21:48:40.11663Z",
          "shell.execute_reply.started": "2025-09-17T21:48:38.285659Z",
          "shell.execute_reply": "2025-09-17T21:48:40.115811Z"
        },
        "id": "iNaNDyjH3le8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "    background: linear-gradient(145deg, rgba(0, 128, 128, 0.98), rgba(255, 127, 127, 0.98));\n",
        "    backdrop-filter: blur(10px);\n",
        "    color: #e6f3ff;\n",
        "    font-size: 2em;\n",
        "    font-family: 'Montserrat', sans-serif;\n",
        "    font-weight: 700;\n",
        "    text-align: center;\n",
        "    border-radius: 30px;\n",
        "    border: 3px solid #000000;\n",
        "    padding: 30px 50px;\n",
        "    margin: 40px auto;\n",
        "    line-height: 1.6;\n",
        "    letter-spacing: 2px;\n",
        "    width: 85%;\n",
        "    text-transform: uppercase;\n",
        "    box-shadow:\n",
        "        0 0 25px rgba(0, 0, 0, 0.6),\n",
        "        0 0 45px rgba(0, 0, 0, 0.35),\n",
        "        inset 0 0 15px rgba(0, 0, 0, 0.3),\n",
        "        0 6px 28px rgba(0, 0, 0, 0.2);\n",
        "    position: relative;\n",
        "    overflow: hidden;\n",
        "    transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);\">\n",
        "    <div style=\"\n",
        "        position: absolute;\n",
        "        top: -50%;\n",
        "        left: -50%;\n",
        "        width: 200%;\n",
        "        height: 200%;\n",
        "        background: radial-gradient(circle, rgba(0, 0, 0, 0.2) 0%, transparent 70%);\n",
        "        animation: rotateGradient 8s infinite ease-in-out;\">\n",
        "    </div>\n",
        "    ⚙️ Full Pipeline Execution\n",
        "</div>\n",
        "\n",
        "<style>\n",
        "div:hover {\n",
        "    transform: translateY(-5px) scale(1.02);\n",
        "    box-shadow:\n",
        "        0 0 35px rgba(0, 0, 0, 0.8),\n",
        "        0 0 60px rgba(0, 0, 0, 0.5),\n",
        "        inset 0 0 20px rgba(0, 0, 0, 0.35),\n",
        "        0 10px 40px rgba(0, 0, 0, 0.25);\n",
        "    border-color: #000000;\n",
        "}\n",
        "\n",
        "@keyframes rotateGradient {\n",
        "    0% { transform: rotate(0deg); opacity: 0.3; }\n",
        "    50% { opacity: 0.5; }\n",
        "    100% { transform: rotate(360deg); opacity: 0.3; }\n",
        "}\n",
        "</style>"
      ],
      "metadata": {
        "id": "XTessVb23le8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import ElasticNet, LinearRegression\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from dataclasses import dataclass, field\n",
        "import kaggle_evaluation.default_inference_server\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ============ PATHS ============\n",
        "DATA_PATH = Path('/kaggle/input/hull-tactical-market-prediction/')\n",
        "\n",
        "# ============ MODEL CONFIGS ============\n",
        "@dataclass\n",
        "class ModelParameters:\n",
        "    enet_alpha: float = 0.01\n",
        "    enet_l1_ratio: float = 0.5\n",
        "    xgb_n_estimators: int = 200\n",
        "    xgb_max_depth: int = 5\n",
        "    xgb_learning_rate: float = 0.05\n",
        "    lgb_n_estimators: int = 200\n",
        "    lgb_max_depth: int = 7\n",
        "    lgb_learning_rate: float = 0.03\n",
        "    ensemble_weights: dict = field(default_factory=lambda: {'enet': 0.25, 'xgb': 0.45, 'lgb': 0.3})\n",
        "    vol_window: int = 20\n",
        "    signal_multiplier: float = 800.0  # Tuned for stronger signal\n",
        "    min_signal: float = 0.0\n",
        "    max_signal: float = 2.0\n",
        "    vol_scaling_low: float = 0.8  # Adjusted\n",
        "    vol_scaling_high: float = 1.6  # Adjusted\n",
        "    retrain_freq: int = 1\n",
        "    missing_threshold: float = 0.5\n",
        "    max_train_rows: int = 1000\n",
        "    max_features: int = 15\n",
        "\n",
        "# Initialize parameters\n",
        "params = ModelParameters()\n",
        "\n",
        "# ============ DATA LOADING AND PREPROCESSING ============\n",
        "def load_trainset() -> pl.DataFrame:\n",
        "    df = (\n",
        "        pl.read_csv(DATA_PATH / \"train.csv\")\n",
        "        .rename({'market_forward_excess_returns': 'target'})\n",
        "        .with_columns(pl.exclude('date_id').cast(pl.Float64, strict=False))\n",
        "        .filter(pl.col('date_id') >= 37)\n",
        "        .tail(params.max_train_rows)\n",
        "    )\n",
        "    missing_counts = {col: df[col].is_null().mean() for col in df.columns}\n",
        "    feature_cols = [\n",
        "        col for col, miss_rate in missing_counts.items()\n",
        "        if miss_rate <= params.missing_threshold and col not in ['date_id', 'target']\n",
        "    ]\n",
        "    keep_cols = ['date_id', 'target'] + feature_cols\n",
        "    if len(keep_cols) != len(set(keep_cols)):\n",
        "        raise ValueError(f\"Duplicate columns in keep_cols: {keep_cols}\")\n",
        "    return df.select(keep_cols)\n",
        "\n",
        "def load_testset() -> pl.DataFrame:\n",
        "    df = (\n",
        "        pl.read_csv(DATA_PATH / \"test.csv\")\n",
        "        .with_columns(pl.exclude('date_id', 'is_scored').cast(pl.Float64, strict=False))\n",
        "    )\n",
        "    train_cols = load_trainset().columns\n",
        "    feature_cols = [col for col in train_cols if col not in ['date_id', 'target']]\n",
        "    return df.select(['date_id', 'is_scored', 'lagged_market_forward_excess_returns'] + feature_cols)\n",
        "\n",
        "def create_features(df: pl.DataFrame, is_train: bool = False, median_values: dict = None) -> pl.DataFrame:\n",
        "    logger.info(f\"Initial columns ({df.height} rows): {df.columns}\")\n",
        "\n",
        "    # Drop existing derived columns to prevent duplicates\n",
        "    derived_cols = [\"U1\", \"U2\", \"V1_S1_interaction\", \"M11_V1_interaction\", \"I9_S1_interaction\", \"P1_lag1\", \"M11_lag1\", \"target_roll_std_5\"]\n",
        "    df = df.drop([col for col in derived_cols if col in df.columns])\n",
        "\n",
        "    feature_prefixes = ['D', 'E', 'I', 'M', 'P', 'S', 'V']\n",
        "    base_features = [col for col in df.columns if any(col.startswith(prefix) for prefix in feature_prefixes)]\n",
        "\n",
        "    # Single with_columns call for all derived features\n",
        "    expressions = []\n",
        "    required_cols = ['I1', 'I2', 'I7', 'I9', 'M11']\n",
        "    if all(col in base_features for col in required_cols):\n",
        "        expressions.extend([\n",
        "            (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n",
        "            (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\")\n",
        "        ])\n",
        "\n",
        "    if 'V1' in base_features and 'S1' in base_features:\n",
        "        expressions.append((pl.col(\"V1\") * pl.col(\"S1\")).alias(\"V1_S1_interaction\"))\n",
        "    if 'M11' in base_features and 'V1' in base_features:\n",
        "        expressions.append((pl.col(\"M11\") * pl.col(\"V1\")).alias(\"M11_V1_interaction\"))\n",
        "    if 'I9' in base_features and 'S1' in base_features:\n",
        "        expressions.append((pl.col(\"I9\") * pl.col(\"S1\")).alias(\"I9_S1_interaction\"))\n",
        "\n",
        "    if is_train:\n",
        "        if 'P1' in base_features:\n",
        "            expressions.append(pl.col(\"P1\").shift(1).alias(\"P1_lag1\"))\n",
        "        if 'M11' in base_features:\n",
        "            expressions.append(pl.col(\"M11\").shift(1).alias(\"M11_lag1\"))\n",
        "        if 'target' in df.columns:\n",
        "            expressions.append(pl.col(\"target\").rolling_std(window_size=5).alias(\"target_roll_std_5\"))\n",
        "\n",
        "    if expressions:\n",
        "        df = df.with_columns(expressions)\n",
        "\n",
        "    # Test-only feature\n",
        "    if not is_train and 'lagged_market_forward_excess_returns' in df.columns:\n",
        "        base_features.append('lagged_market_forward_excess_returns')\n",
        "\n",
        "    # Impute missing values\n",
        "    for col in base_features:\n",
        "        if col.startswith('I'):\n",
        "            df = df.with_columns(pl.col(col).fill_null(pl.col(col).forward_fill()).fill_null(pl.col(col).backward_fill()))\n",
        "        median = median_values.get(col, df[col].median()) if median_values else df[col].median()\n",
        "        df = df.with_columns(pl.col(col).fill_null(median if median is not None else 0.0))\n",
        "\n",
        "    # Impute derived and additional features\n",
        "    derived_features = [\"U1\", \"U2\", \"V1_S1_interaction\", \"M11_V1_interaction\", \"I9_S1_interaction\"]\n",
        "    additional_features = [\"P1_lag1\", \"M11_lag1\", \"target_roll_std_5\"] if is_train else []\n",
        "    for col in derived_features + additional_features:\n",
        "        if col in df.columns:\n",
        "            median = median_values.get(col, df[col].median()) if median_values else df[col].median()\n",
        "            df = df.with_columns(pl.col(col).fill_null(median if median is not None else 0.0))\n",
        "\n",
        "    # Ensure unique columns\n",
        "    df = df.select([pl.col(col).alias(col) for col in df.columns])\n",
        "\n",
        "    # Check for duplicate columns\n",
        "    all_cols = df.columns\n",
        "    if len(all_cols) != len(set(all_cols)):\n",
        "        duplicates = [col for col in set(all_cols) if all_cols.count(col) > 1]\n",
        "        logger.error(f\"Duplicate columns detected: {duplicates}\")\n",
        "        raise ValueError(f\"Duplicate columns detected: {duplicates}\")\n",
        "\n",
        "    logger.info(f\"Final columns ({df.height} rows): {df.columns}\")\n",
        "\n",
        "    # Feature list (exclude training-only features)\n",
        "    features = base_features + [col for col in derived_features if col in df.columns]\n",
        "    select_cols = [\"date_id\"] + features + ([\"target\"] if is_train else [])\n",
        "    return df.select(select_cols)\n",
        "\n",
        "# ============ MODEL TRAINING ============\n",
        "start_time = time.time()\n",
        "train = load_trainset()\n",
        "train = create_features(train, is_train=True)\n",
        "features = [col for col in train.columns if col not in ['date_id', 'target', 'P1_lag1', 'M11_lag1', 'target_roll_std_5']]\n",
        "\n",
        "# Cache median values for imputation\n",
        "median_values = {col: train[col].median() if col in train.columns and train[col].is_null().mean() < 1.0 else 0.0 for col in features}\n",
        "\n",
        "# Check for NaNs\n",
        "X_train = train.select(features).to_pandas()\n",
        "if X_train.isna().any().any():\n",
        "    raise ValueError(f\"NaNs found in X_train for columns: {X_train.columns[X_train.isna().any()].tolist()}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "y_train = train['target'].to_pandas()\n",
        "\n",
        "# Train individual models\n",
        "enet_model = ElasticNet(alpha=params.enet_alpha, l1_ratio=params.enet_l1_ratio, max_iter=1000000)\n",
        "enet_model.fit(X_train, y_train)\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=params.xgb_n_estimators,\n",
        "    max_depth=params.xgb_max_depth,\n",
        "    learning_rate=params.xgb_learning_rate,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    n_estimators=params.lgb_n_estimators,\n",
        "    max_depth=params.lgb_max_depth,\n",
        "    learning_rate=params.lgb_learning_rate,\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Feature selection based on XGBoost importance\n",
        "feature_importance = xgb_model.feature_importances_\n",
        "feature_ranking = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\n",
        "features = [f[0] for f in feature_ranking[:params.max_features]]\n",
        "\n",
        "# Retrain with selected features\n",
        "X_train = train.select(features).to_pandas()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "enet_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Train meta-learner\n",
        "meta_features = np.column_stack([\n",
        "    enet_model.predict(X_train),\n",
        "    xgb_model.predict(X_train),\n",
        "    lgb_model.predict(X_train)\n",
        "])\n",
        "meta_model = LinearRegression()\n",
        "meta_model.fit(meta_features, y_train)\n",
        "\n",
        "# Check startup time\n",
        "if time.time() - start_time > 900:\n",
        "    raise RuntimeError(\"Startup time exceeded 900 seconds\")\n",
        "\n",
        "# State for online learning\n",
        "previous_lagged = None\n",
        "test_row_count = 0\n",
        "last_allocation = 0.0\n",
        "v1_median = train['V1'].median() if 'V1' in train.columns else 0.0\n",
        "\n",
        "# ============ VOLATILITY ESTIMATION ============\n",
        "def estimate_volatility(test: pl.DataFrame, train: pl.DataFrame) -> float:\n",
        "    vol = test['V1'][0] if 'V1' in test.columns else (train['target'].tail(params.vol_window).std() or 0.01)\n",
        "    recent_returns = train['target'].tail(params.vol_window).to_numpy()\n",
        "    if len(recent_returns) > 1:\n",
        "        garch_vol = np.sqrt(0.3 * np.var(recent_returns) + 0.7 * vol**2)\n",
        "        return max(garch_vol, 0.01)\n",
        "    return max(vol, 0.01)\n",
        "\n",
        "# ============ PREDICTION FUNCTION ============\n",
        "def predict(test: pl.DataFrame) -> float:\n",
        "    global previous_lagged, train, enet_model, xgb_model, lgb_model, meta_model, scaler, test_row_count, last_allocation, v1_median, features, median_values\n",
        "\n",
        "    # Online learning: Update training data\n",
        "    if previous_lagged is not None and 'lagged_market_forward_excess_returns' in previous_lagged.columns:\n",
        "        append_row = previous_lagged.with_columns(\n",
        "            pl.col('lagged_market_forward_excess_returns').alias('target')\n",
        "        )\n",
        "        # Drop derived columns before feature creation\n",
        "        append_row = append_row.drop([col for col in [\"U1\", \"U2\", \"V1_S1_interaction\", \"M11_V1_interaction\", \"I9_S1_interaction\"] if col in append_row.columns])\n",
        "        append_row = create_features(append_row, is_train=False, median_values=median_values)\n",
        "        if append_row.height > 0:\n",
        "            # Align columns with train\n",
        "            missing_cols = [col for col in train.columns if col not in append_row.columns]\n",
        "            expressions = [pl.lit(median_values.get(col, 0.0)).cast(pl.Float64).alias(col) for col in missing_cols]\n",
        "            if expressions:\n",
        "                append_row = append_row.with_columns(expressions)\n",
        "            append_row = append_row.select(train.columns)  # Ensure exact column match\n",
        "            logger.info(f\"Appending row with shape {append_row.shape} to train with shape {train.shape}\")\n",
        "            train = train.vstack(append_row)\n",
        "            if train.height > params.max_train_rows:\n",
        "                train = train.tail(params.max_train_rows)\n",
        "\n",
        "        # Retrain every `retrain_freq` rows\n",
        "        if test_row_count % params.retrain_freq == 0:\n",
        "            X_train = scaler.fit_transform(train.select(features).to_pandas())\n",
        "            y_train = train['target'].to_pandas()\n",
        "            if y_train.isna().any():\n",
        "                raise ValueError(\"NaNs found in y_train during retraining\")\n",
        "            enet_model.fit(X_train, y_train)\n",
        "            xgb_model.fit(X_train, y_train)\n",
        "            lgb_model.fit(X_train, y_train)\n",
        "            meta_features = np.column_stack([\n",
        "                enet_model.predict(X_train),\n",
        "                xgb_model.predict(X_train),\n",
        "                lgb_model.predict(X_train)\n",
        "            ])\n",
        "            meta_model.fit(meta_features, y_train)\n",
        "\n",
        "    # Preprocess test data\n",
        "    test = test.drop([col for col in [\"U1\", \"U2\", \"V1_S1_interaction\", \"M11_V1_interaction\", \"I9_S1_interaction\"] if col in test.columns])\n",
        "    test = create_features(test, is_train=False, median_values=median_values)\n",
        "\n",
        "    # Ensure no NaNs in test data\n",
        "    X_test = test.select(features).to_pandas()\n",
        "    if X_test.isna().any().any():\n",
        "        raise ValueError(f\"NaNs found in X_test for columns: {X_test.columns[X_test.isna().any()].tolist()}\")\n",
        "\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Ensemble prediction with meta-learner\n",
        "    meta_features = np.column_stack([\n",
        "        enet_model.predict(X_test),\n",
        "        xgb_model.predict(X_test),\n",
        "        lgb_model.predict(X_test)\n",
        "    ])\n",
        "    raw_pred = meta_model.predict(meta_features)[0]\n",
        "\n",
        "    # Estimate volatility and dynamic vol_scaling\n",
        "    vol = estimate_volatility(test, train)\n",
        "    vol_scaling = params.vol_scaling_low if ('V1' in test.columns and test['V1'][0] < v1_median) else params.vol_scaling_high\n",
        "\n",
        "    # Convert to signal\n",
        "    signal = np.clip(\n",
        "        raw_pred * params.signal_multiplier,\n",
        "        params.min_signal,\n",
        "        params.max_signal\n",
        "    )\n",
        "\n",
        "    # Volatility-adjusted allocation\n",
        "    allocation = min(params.max_signal, max(params.min_signal, signal / (vol * vol_scaling)))\n",
        "\n",
        "    # Smooth allocation\n",
        "    transaction_cost = 0.00004\n",
        "    allocation = (0.8 * allocation + 0.2 * last_allocation) * (1 - transaction_cost)\n",
        "    last_allocation = allocation\n",
        "\n",
        "    # Update state\n",
        "    previous_lagged = test\n",
        "    test_row_count += 1\n",
        "\n",
        "    return float(allocation)\n",
        "\n",
        "# ============ LAUNCH SERVER ============\n",
        "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
        "\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    inference_server.serve()\n",
        "else:\n",
        "    inference_server.run_local_gateway((str(DATA_PATH),))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T22:00:15.621346Z",
          "iopub.execute_input": "2025-09-17T22:00:15.62212Z",
          "iopub.status.idle": "2025-09-17T22:00:24.565873Z",
          "shell.execute_reply.started": "2025-09-17T22:00:15.622093Z",
          "shell.execute_reply": "2025-09-17T22:00:24.56513Z"
        },
        "id": "fpcVF3Ef3le8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "    background: linear-gradient(145deg, rgba(0, 128, 128, 0.98), rgba(255, 127, 127, 0.98));\n",
        "    backdrop-filter: blur(10px);\n",
        "    color: #e6f3ff;\n",
        "    font-size: 2em;\n",
        "    font-family: 'Montserrat', sans-serif;\n",
        "    font-weight: 700;\n",
        "    text-align: center;\n",
        "    border-radius: 30px;\n",
        "    border: 3px solid #000000;\n",
        "    padding: 30px 50px;\n",
        "    margin: 40px auto;\n",
        "    line-height: 1.6;\n",
        "    letter-spacing: 2px;\n",
        "    width: 85%;\n",
        "    text-transform: uppercase;\n",
        "    box-shadow:\n",
        "        0 0 25px rgba(0, 0, 0, 0.6),\n",
        "        0 0 45px rgba(0, 0, 0, 0.35),\n",
        "        inset 0 0 15px rgba(0, 0, 0, 0.3),\n",
        "        0 6px 28px rgba(0, 0, 0, 0.2);\n",
        "    position: relative;\n",
        "    overflow: hidden;\n",
        "    transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);\">\n",
        "    <div style=\"\n",
        "        position: absolute;\n",
        "        top: -50%;\n",
        "        left: -50%;\n",
        "        width: 200%;\n",
        "        height: 200%;\n",
        "        background: radial-gradient(circle, rgba(0, 0, 0, 0.2) 0%, transparent 70%);\n",
        "        animation: rotateGradient 8s infinite ease-in-out;\">\n",
        "    </div>\n",
        "    👍 If You Liked My Work, Kindly Share and Upvote\n",
        "</div>\n",
        "\n",
        "<style>\n",
        "div:hover {\n",
        "    transform: translateY(-5px) scale(1.02);\n",
        "    box-shadow:\n",
        "        0 0 35px rgba(0, 0, 0, 0.8),\n",
        "        0 0 60px rgba(0, 0, 0, 0.5),\n",
        "        inset 0 0 20px rgba(0, 0, 0, 0.35),\n",
        "        0 10px 40px rgba(0, 0, 0, 0.25);\n",
        "    border-color: #000000;\n",
        "}\n",
        "\n",
        "@keyframes rotateGradient {\n",
        "    0% { transform: rotate(0deg); opacity: 0.3; }\n",
        "    50% { opacity: 0.5; }\n",
        "    100% { transform: rotate(360deg); opacity: 0.3; }\n",
        "}\n",
        "</style>"
      ],
      "metadata": {
        "id": "BBPyp1L63le9"
      }
    }
  ]
}